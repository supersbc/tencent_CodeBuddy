#!/usr/bin/env python3
"""
é«˜çº§æ–‡ä»¶å¤„ç†å™¨ - æ”¯æŒExcelã€PDFã€å›¾ç‰‡ç­‰å¤šç§æ ¼å¼
æ™ºèƒ½è¯†åˆ«å¤§ç¯å¢ƒå†…çš„å¤šä¸ªç³»ç»Ÿå’Œå¤æ‚éƒ¨ç½²æ¶æ„
"""

import os
import re
import json
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime

# Excelå¤„ç†
try:
    import pandas as pd
    import openpyxl
    from openpyxl import load_workbook
    EXCEL_AVAILABLE = True
except ImportError:
    EXCEL_AVAILABLE = False
    print("âš ï¸  Excelå¤„ç†åº“æœªå®‰è£…")

# PDFå¤„ç†
try:
    import PyPDF2
    import pdfplumber
    PDF_AVAILABLE = True
except ImportError:
    PDF_AVAILABLE = False
    print("âš ï¸  PDFå¤„ç†åº“æœªå®‰è£…")

# å›¾åƒå¤„ç†
try:
    from PIL import Image
    import pytesseract
    import cv2
    import numpy as np
    IMAGE_AVAILABLE = True
except ImportError:
    IMAGE_AVAILABLE = False
    print("âš ï¸  å›¾åƒå¤„ç†åº“æœªå®‰è£…")

# OCRå¢å¼º
try:
    import easyocr
    EASYOCR_AVAILABLE = True
except ImportError:
    EASYOCR_AVAILABLE = False


@dataclass
class SystemInfo:
    """å•ä¸ªç³»ç»Ÿä¿¡æ¯"""
    system_name: str = ""
    system_type: str = ""  # æ ¸å¿ƒç³»ç»Ÿã€è¾…åŠ©ç³»ç»Ÿã€å¤–å›´ç³»ç»Ÿ
    business_module: str = ""  # ä¸šåŠ¡æ¨¡å—
    data_size_gb: float = 0.0
    table_count: int = 0
    qps: int = 0
    tps: int = 0
    peak_qps: int = 0
    connections: int = 0
    availability_requirement: str = "99.9%"
    data_sensitivity: str = "ä¸­"
    backup_requirement: str = "æ¯æ—¥"
    notes: str = ""


@dataclass
class DeploymentTopology:
    """éƒ¨ç½²æ‹“æ‰‘ä¿¡æ¯"""
    deployment_mode: str = "å•ä¸­å¿ƒ"  # å•ä¸­å¿ƒã€åŒåŸåŒä¸­å¿ƒã€åŒåŸå¤šä¸­å¿ƒã€ä¸¤åœ°ä¸‰ä¸­å¿ƒã€ä¸‰åœ°äº”ä¸­å¿ƒã€å¤šåœ°å¤šä¸­å¿ƒ
    primary_region: str = ""  # ä¸»ä¸­å¿ƒåŒºåŸŸ
    disaster_regions: List[str] = field(default_factory=list)  # ç¾å¤‡ä¸­å¿ƒåŒºåŸŸ
    
    # åŒåŸéƒ¨ç½²
    same_city_centers: int = 1  # åŒåŸä¸­å¿ƒæ•°é‡
    same_city_distance_km: float = 0.0  # åŒåŸä¸­å¿ƒé—´è·ç¦»
    same_city_network_latency_ms: float = 0.0  # åŒåŸç½‘ç»œå»¶è¿Ÿ
    
    # å¼‚åœ°éƒ¨ç½²
    remote_centers: int = 0  # å¼‚åœ°ä¸­å¿ƒæ•°é‡
    remote_distance_km: float = 0.0  # å¼‚åœ°è·ç¦»
    remote_network_latency_ms: float = 0.0  # å¼‚åœ°ç½‘ç»œå»¶è¿Ÿ
    
    # æ•°æ®åŒæ­¥
    sync_mode: str = "å¼‚æ­¥"  # åŒæ­¥ã€å¼‚æ­¥ã€åŠåŒæ­¥
    rpo_seconds: int = 0  # æ¢å¤ç‚¹ç›®æ ‡ï¼ˆç§’ï¼‰
    rto_seconds: int = 0  # æ¢å¤æ—¶é—´ç›®æ ‡ï¼ˆç§’ï¼‰
    
    # å®¹ç¾ç­–ç•¥
    disaster_recovery_type: str = "å†·å¤‡"  # å†·å¤‡ã€æ¸©å¤‡ã€çƒ­å¤‡ã€åŒæ´»ã€å¤šæ´»
    auto_failover: bool = False  # è‡ªåŠ¨æ•…éšœåˆ‡æ¢
    failover_time_seconds: int = 0  # æ•…éšœåˆ‡æ¢æ—¶é—´
    
    # ç½‘ç»œæ¶æ„
    network_architecture: str = "ä¸“çº¿"  # ä¸“çº¿ã€VPNã€å…¬ç½‘
    bandwidth_mbps: int = 0  # å¸¦å®½


@dataclass
class MultiSystemEnvironment:
    """å¤šç³»ç»Ÿå¤§ç¯å¢ƒ"""
    environment_name: str = ""
    total_systems: int = 0
    systems: List[SystemInfo] = field(default_factory=list)
    deployment: DeploymentTopology = field(default_factory=DeploymentTopology)
    
    # æ•´ä½“ç»Ÿè®¡
    total_data_size_tb: float = 0.0
    total_qps: int = 0
    total_tps: int = 0
    total_connections: int = 0
    
    # ä¸šåŠ¡ç‰¹å¾
    business_peak_hours: str = ""  # ä¸šåŠ¡é«˜å³°æ—¶æ®µ
    seasonal_peak: str = ""  # å­£èŠ‚æ€§é«˜å³°
    growth_rate_yearly: float = 0.0  # å¹´å¢é•¿ç‡
    
    # åˆè§„è¦æ±‚
    compliance_requirements: List[str] = field(default_factory=list)
    data_residency: str = ""  # æ•°æ®é©»ç•™è¦æ±‚


class AdvancedFileProcessor:
    """é«˜çº§æ–‡ä»¶å¤„ç†å™¨"""
    
    def __init__(self):
        self.supported_formats = {
            'excel': ['.xlsx', '.xls', '.xlsm', '.xlsb'],
            'pdf': ['.pdf'],
            'image': ['.png', '.jpg', '.jpeg', '.bmp', '.tiff', '.gif'],
            'text': ['.txt', '.csv', '.json']
        }
        
        # åˆå§‹åŒ–OCRå¼•æ“
        self.easyocr_reader = None
        if EASYOCR_AVAILABLE:
            try:
                self.easyocr_reader = easyocr.Reader(['ch_sim', 'en'])
            except:
                pass
        
        # å…³é”®è¯æ˜ å°„
        self.keywords = self._init_keywords()
    
    def _init_keywords(self) -> Dict[str, List[str]]:
        """åˆå§‹åŒ–å…³é”®è¯æ˜ å°„"""
        return {
            # ç³»ç»Ÿè¯†åˆ«
            'system_name': ['ç³»ç»Ÿåç§°', 'ç³»ç»Ÿ', 'system', 'æ¨¡å—', 'module', 'åº”ç”¨', 'application'],
            'system_type': ['ç³»ç»Ÿç±»å‹', 'ç±»å‹', 'type', 'æ ¸å¿ƒ', 'è¾…åŠ©', 'å¤–å›´'],
            
            # æ•°æ®é‡
            'data_size': ['æ•°æ®é‡', 'æ•°æ®å¤§å°', 'å®¹é‡', 'data size', 'capacity', 'storage', 'GB', 'TB', 'PB'],
            'table_count': ['è¡¨æ•°é‡', 'è¡¨æ•°', 'è¡¨ä¸ªæ•°', 'table count', 'tables'],
            'database_count': ['åº“æ•°é‡', 'æ•°æ®åº“æ•°', 'database count', 'databases'],
            
            # æ€§èƒ½æŒ‡æ ‡
            'qps': ['QPS', 'qps', 'æ¯ç§’æŸ¥è¯¢', 'queries per second', 'æŸ¥è¯¢æ•°'],
            'tps': ['TPS', 'tps', 'æ¯ç§’äº‹åŠ¡', 'transactions per second', 'äº‹åŠ¡æ•°'],
            'peak_qps': ['å³°å€¼QPS', 'é«˜å³°QPS', 'peak qps', 'æœ€å¤§QPS'],
            'connections': ['è¿æ¥æ•°', 'å¹¶å‘è¿æ¥', 'connections', 'concurrent'],
            'response_time': ['å“åº”æ—¶é—´', 'å»¶è¿Ÿ', 'response time', 'latency', 'RT'],
            
            # å¯ç”¨æ€§
            'availability': ['å¯ç”¨æ€§', 'é«˜å¯ç”¨', 'availability', 'HA', 'SLA'],
            'rpo': ['RPO', 'rpo', 'æ¢å¤ç‚¹', 'recovery point'],
            'rto': ['RTO', 'rto', 'æ¢å¤æ—¶é—´', 'recovery time'],
            
            # éƒ¨ç½²æ–¹å¼
            'deployment_mode': ['éƒ¨ç½²æ–¹å¼', 'éƒ¨ç½²æ¨¡å¼', 'deployment', 'æ¶æ„æ¨¡å¼'],
            'same_city': ['åŒåŸ', 'æœ¬åœ°', 'same city', 'local'],
            'remote': ['å¼‚åœ°', 'è¿œç¨‹', 'remote', 'disaster'],
            'multi_center': ['å¤šä¸­å¿ƒ', 'åŒä¸­å¿ƒ', 'ä¸‰ä¸­å¿ƒ', 'multi-center', 'dual-center'],
            'two_site_three_center': ['ä¸¤åœ°ä¸‰ä¸­å¿ƒ', '2åœ°3ä¸­å¿ƒ'],
            'active_active': ['åŒæ´»', 'å¤šæ´»', 'active-active', 'multi-active'],
            
            # æ•°æ®åŒæ­¥
            'sync_mode': ['åŒæ­¥æ–¹å¼', 'åŒæ­¥æ¨¡å¼', 'sync mode', 'å¤åˆ¶'],
            'async': ['å¼‚æ­¥', 'async', 'asynchronous'],
            'sync': ['åŒæ­¥', 'sync', 'synchronous'],
            'semi_sync': ['åŠåŒæ­¥', 'semi-sync', 'semi-synchronous'],
            
            # å¤‡ä»½
            'backup': ['å¤‡ä»½', 'backup', 'å¤‡ä»½ç­–ç•¥'],
            'backup_frequency': ['å¤‡ä»½é¢‘ç‡', 'backup frequency'],
            
            # å®‰å…¨åˆè§„
            'security': ['å®‰å…¨', 'security', 'åŠ å¯†', 'encryption'],
            'compliance': ['åˆè§„', 'compliance', 'ç­‰ä¿', 'åˆ†ä¿'],
            'data_sensitivity': ['æ•æ„Ÿåº¦', 'æ•°æ®æ•æ„Ÿ', 'sensitivity'],
        }
    
    def process_file(self, file_path: str) -> Dict[str, Any]:
        """
        å¤„ç†æ–‡ä»¶å¹¶æå–ä¿¡æ¯
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            æå–çš„ä¿¡æ¯å­—å…¸
        """
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        file_ext = os.path.splitext(file_path)[1].lower()
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹è°ƒç”¨ä¸åŒçš„å¤„ç†æ–¹æ³•
        if file_ext in self.supported_formats['excel']:
            return self.process_excel(file_path)
        elif file_ext in self.supported_formats['pdf']:
            return self.process_pdf(file_path)
        elif file_ext in self.supported_formats['image']:
            return self.process_image(file_path)
        elif file_ext in self.supported_formats['text']:
            return self.process_text(file_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}")
    
    def process_excel(self, file_path: str) -> Dict[str, Any]:
        """
        å¤„ç†Excelæ–‡ä»¶
        æ”¯æŒå¤šä¸ªå·¥ä½œè¡¨ï¼Œæ™ºèƒ½è¯†åˆ«ç³»ç»Ÿä¿¡æ¯å’Œéƒ¨ç½²æ¶æ„
        """
        if not EXCEL_AVAILABLE:
            raise ImportError("Excelå¤„ç†åº“æœªå®‰è£…ï¼Œè¯·å®‰è£…: pip install pandas openpyxl")
        
        result = {
            'file_type': 'excel',
            'file_path': file_path,
            'systems': [],
            'deployment': {},
            'summary': {},
            'raw_data': {}
        }
        
        try:
            # è¯»å–æ‰€æœ‰å·¥ä½œè¡¨
            excel_file = pd.ExcelFile(file_path)
            workbook = load_workbook(file_path)
            
            for sheet_name in excel_file.sheet_names:
                df = pd.read_excel(file_path, sheet_name=sheet_name)
                result['raw_data'][sheet_name] = df.to_dict('records')
                
                # æ™ºèƒ½è¯†åˆ«å·¥ä½œè¡¨ç±»å‹
                sheet_type = self._identify_sheet_type(sheet_name, df)
                
                if sheet_type == 'system_list':
                    # ç³»ç»Ÿæ¸…å•è¡¨
                    systems = self._extract_systems_from_df(df)
                    result['systems'].extend(systems)
                
                elif sheet_type == 'deployment':
                    # éƒ¨ç½²æ¶æ„è¡¨
                    deployment = self._extract_deployment_from_df(df)
                    result['deployment'].update(deployment)
                
                elif sheet_type == 'summary':
                    # æ±‡æ€»è¡¨
                    summary = self._extract_summary_from_df(df)
                    result['summary'].update(summary)
            
            # è®¡ç®—æ•´ä½“ç»Ÿè®¡
            result['statistics'] = self._calculate_statistics(result['systems'])
            
            # æ™ºèƒ½æ¨æ–­éƒ¨ç½²æ–¹å¼
            if not result['deployment']:
                result['deployment'] = self._infer_deployment(result['systems'], result['summary'])
            
        except Exception as e:
            result['error'] = str(e)
            result['success'] = False
        else:
            result['success'] = True
        
        return result
    
    def process_pdf(self, file_path: str) -> Dict[str, Any]:
        """
        å¤„ç†PDFæ–‡ä»¶
        æå–æ–‡æœ¬å’Œè¡¨æ ¼ä¿¡æ¯
        """
        if not PDF_AVAILABLE:
            raise ImportError("PDFå¤„ç†åº“æœªå®‰è£…ï¼Œè¯·å®‰è£…: pip install PyPDF2 pdfplumber")
        
        result = {
            'file_type': 'pdf',
            'file_path': file_path,
            'systems': [],
            'deployment': {},
            'text_content': '',
            'tables': []
        }
        
        try:
            # ä½¿ç”¨pdfplumberæå–è¡¨æ ¼
            with pdfplumber.open(file_path) as pdf:
                all_text = []
                
                for page_num, page in enumerate(pdf.pages):
                    # æå–æ–‡æœ¬
                    text = page.extract_text()
                    if text:
                        all_text.append(text)
                    
                    # æå–è¡¨æ ¼
                    tables = page.extract_tables()
                    for table in tables:
                        if table:
                            # è½¬æ¢ä¸ºDataFrame
                            df = pd.DataFrame(table[1:], columns=table[0])
                            result['tables'].append({
                                'page': page_num + 1,
                                'data': df.to_dict('records')
                            })
                            
                            # å°è¯•ä»è¡¨æ ¼æå–ç³»ç»Ÿä¿¡æ¯
                            systems = self._extract_systems_from_df(df)
                            result['systems'].extend(systems)
                
                result['text_content'] = '\n'.join(all_text)
            
            # ä»æ–‡æœ¬ä¸­æå–éƒ¨ç½²ä¿¡æ¯
            result['deployment'] = self._extract_deployment_from_text(result['text_content'])
            
            # è®¡ç®—ç»Ÿè®¡
            result['statistics'] = self._calculate_statistics(result['systems'])
            
        except Exception as e:
            result['error'] = str(e)
            result['success'] = False
        else:
            result['success'] = True
        
        return result
    
    def process_image(self, file_path: str) -> Dict[str, Any]:
        """
        å¤„ç†å›¾ç‰‡æ–‡ä»¶
        ä½¿ç”¨OCRæå–æ–‡æœ¬å’Œè¡¨æ ¼
        """
        if not IMAGE_AVAILABLE:
            raise ImportError("å›¾åƒå¤„ç†åº“æœªå®‰è£…")
        
        result = {
            'file_type': 'image',
            'file_path': file_path,
            'systems': [],
            'deployment': {},
            'ocr_text': ''
        }
        
        try:
            # è¯»å–å›¾åƒ
            image = Image.open(file_path)
            
            # å›¾åƒé¢„å¤„ç†
            processed_image = self._preprocess_image(image)
            
            # OCRè¯†åˆ«
            if self.easyocr_reader:
                # ä½¿ç”¨EasyOCRï¼ˆæ›´å‡†ç¡®ï¼‰
                ocr_result = self.easyocr_reader.readtext(np.array(processed_image))
                result['ocr_text'] = '\n'.join([text[1] for text in ocr_result])
            else:
                # ä½¿ç”¨Tesseract
                result['ocr_text'] = pytesseract.image_to_string(processed_image, lang='chi_sim+eng')
            
            # å°è¯•è¯†åˆ«è¡¨æ ¼ç»“æ„
            tables = self._extract_tables_from_image(processed_image)
            result['tables'] = tables
            
            # ä»OCRæ–‡æœ¬æå–ä¿¡æ¯
            result['systems'] = self._extract_systems_from_text(result['ocr_text'])
            result['deployment'] = self._extract_deployment_from_text(result['ocr_text'])
            
            # è®¡ç®—ç»Ÿè®¡
            result['statistics'] = self._calculate_statistics(result['systems'])
            
        except Exception as e:
            result['error'] = str(e)
            result['success'] = False
        else:
            result['success'] = True
        
        return result
    
    def process_text(self, file_path: str) -> Dict[str, Any]:
        """å¤„ç†æ–‡æœ¬æ–‡ä»¶"""
        result = {
            'file_type': 'text',
            'file_path': file_path,
            'systems': [],
            'deployment': {}
        }
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            result['text_content'] = content
            
            # å¦‚æœæ˜¯JSONæ ¼å¼
            if file_path.endswith('.json'):
                try:
                    data = json.loads(content)
                    result['json_data'] = data
                    # ä»JSONæå–ä¿¡æ¯
                    if isinstance(data, dict):
                        if 'systems' in data:
                            result['systems'] = data['systems']
                        if 'deployment' in data:
                            result['deployment'] = data['deployment']
                except:
                    pass
            
            # ä»æ–‡æœ¬æå–ä¿¡æ¯
            if not result['systems']:
                result['systems'] = self._extract_systems_from_text(content)
            if not result['deployment']:
                result['deployment'] = self._extract_deployment_from_text(content)
            
            result['statistics'] = self._calculate_statistics(result['systems'])
            result['success'] = True
            
        except Exception as e:
            result['error'] = str(e)
            result['success'] = False
        
        return result
    
    def _identify_sheet_type(self, sheet_name: str, df: pd.DataFrame) -> str:
        """è¯†åˆ«å·¥ä½œè¡¨ç±»å‹"""
        sheet_name_lower = sheet_name.lower()
        
        # æ£€æŸ¥å·¥ä½œè¡¨åç§°
        if any(keyword in sheet_name_lower for keyword in ['ç³»ç»Ÿ', 'system', 'æ¸…å•', 'list']):
            return 'system_list'
        elif any(keyword in sheet_name_lower for keyword in ['éƒ¨ç½²', 'deployment', 'æ¶æ„', 'architecture']):
            return 'deployment'
        elif any(keyword in sheet_name_lower for keyword in ['æ±‡æ€»', 'summary', 'æ€»è®¡', 'total']):
            return 'summary'
        
        # æ£€æŸ¥åˆ—å
        if not df.empty:
            columns_str = ' '.join(df.columns.astype(str)).lower()
            if any(keyword in columns_str for keyword in ['ç³»ç»Ÿåç§°', 'system name', 'æ¨¡å—']):
                return 'system_list'
            elif any(keyword in columns_str for keyword in ['éƒ¨ç½²', 'deployment', 'ä¸­å¿ƒ', 'center']):
                return 'deployment'
        
        return 'unknown'
    
    def _extract_systems_from_df(self, df: pd.DataFrame) -> List[Dict[str, Any]]:
        """ä»DataFrameæå–ç³»ç»Ÿä¿¡æ¯"""
        systems = []
        
        if df.empty:
            return systems
        
        # åˆ—åæ˜ å°„
        column_mapping = self._map_columns(df.columns)
        
        for idx, row in df.iterrows():
            system = {}
            
            for col_name, mapped_name in column_mapping.items():
                value = row.get(col_name)
                if pd.notna(value):
                    # æ•°æ®æ¸…æ´—å’Œè½¬æ¢
                    system[mapped_name] = self._clean_value(value, mapped_name)
            
            if system:  # åªæ·»åŠ éç©ºç³»ç»Ÿ
                systems.append(system)
        
        return systems
    
    def _extract_deployment_from_df(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ä»DataFrameæå–éƒ¨ç½²ä¿¡æ¯"""
        deployment = {}
        
        # æŸ¥æ‰¾éƒ¨ç½²ç›¸å…³çš„è¡Œ
        for idx, row in df.iterrows():
            for col in df.columns:
                value = str(row[col]).lower()
                
                # è¯†åˆ«éƒ¨ç½²æ¨¡å¼
                if 'ä¸¤åœ°ä¸‰ä¸­å¿ƒ' in value or '2åœ°3ä¸­å¿ƒ' in value:
                    deployment['deployment_mode'] = 'ä¸¤åœ°ä¸‰ä¸­å¿ƒ'
                elif 'åŒåŸåŒä¸­å¿ƒ' in value or 'åŒåŸ2ä¸­å¿ƒ' in value:
                    deployment['deployment_mode'] = 'åŒåŸåŒä¸­å¿ƒ'
                elif 'åŒåŸå¤šä¸­å¿ƒ' in value:
                    deployment['deployment_mode'] = 'åŒåŸå¤šä¸­å¿ƒ'
                elif 'ä¸‰åœ°äº”ä¸­å¿ƒ' in value or '3åœ°5ä¸­å¿ƒ' in value:
                    deployment['deployment_mode'] = 'ä¸‰åœ°äº”ä¸­å¿ƒ'
                elif 'åŒæ´»' in value or 'active-active' in value:
                    deployment['disaster_recovery_type'] = 'åŒæ´»'
                elif 'å¤šæ´»' in value or 'multi-active' in value:
                    deployment['disaster_recovery_type'] = 'å¤šæ´»'
                
                # è¯†åˆ«RPO/RTO
                rpo_match = re.search(r'RPO[ï¼š:=\s]*(\d+)\s*(ç§’|åˆ†|å°æ—¶|s|m|h)', value, re.IGNORECASE)
                if rpo_match:
                    deployment['rpo'] = rpo_match.group(1) + rpo_match.group(2)
                
                rto_match = re.search(r'RTO[ï¼š:=\s]*(\d+)\s*(ç§’|åˆ†|å°æ—¶|s|m|h)', value, re.IGNORECASE)
                if rto_match:
                    deployment['rto'] = rto_match.group(1) + rto_match.group(2)
        
        return deployment
    
    def _extract_summary_from_df(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ä»DataFrameæå–æ±‡æ€»ä¿¡æ¯"""
        summary = {}
        
        for idx, row in df.iterrows():
            for col in df.columns:
                col_lower = str(col).lower()
                value = row[col]
                
                if pd.notna(value):
                    if 'æ€»æ•°æ®é‡' in col_lower or 'total data' in col_lower:
                        summary['total_data_size'] = self._parse_size(str(value))
                    elif 'æ€»qps' in col_lower or 'total qps' in col_lower:
                        summary['total_qps'] = self._parse_number(str(value))
                    elif 'ç³»ç»Ÿæ•°' in col_lower or 'system count' in col_lower:
                        summary['total_systems'] = self._parse_number(str(value))
        
        return summary
    
    def _extract_systems_from_text(self, text: str) -> List[Dict[str, Any]]:
        """ä»æ–‡æœ¬æå–ç³»ç»Ÿä¿¡æ¯"""
        systems = []
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾ç³»ç»Ÿä¿¡æ¯
        # è¿™é‡Œå¯ä»¥æ ¹æ®å®é™…æ–‡æœ¬æ ¼å¼è°ƒæ•´
        lines = text.split('\n')
        
        current_system = {}
        for line in lines:
            line = line.strip()
            if not line:
                if current_system:
                    systems.append(current_system)
                    current_system = {}
                continue
            
            # å°è¯•æå–é”®å€¼å¯¹
            if 'ï¼š' in line or ':' in line:
                parts = re.split('[ï¼š:]', line, 1)
                if len(parts) == 2:
                    key, value = parts
                    key = key.strip()
                    value = value.strip()
                    
                    # æ˜ å°„åˆ°æ ‡å‡†å­—æ®µ
                    if 'ç³»ç»Ÿåç§°' in key or 'system name' in key.lower():
                        current_system['system_name'] = value
                    elif 'qps' in key.lower():
                        current_system['qps'] = self._parse_number(value)
                    elif 'æ•°æ®é‡' in key or 'data size' in key.lower():
                        current_system['data_size_gb'] = self._parse_size(value)
        
        if current_system:
            systems.append(current_system)
        
        return systems
    
    def _extract_deployment_from_text(self, text: str) -> Dict[str, Any]:
        """ä»æ–‡æœ¬æå–éƒ¨ç½²ä¿¡æ¯"""
        deployment = {}
        
        text_lower = text.lower()
        
        # è¯†åˆ«éƒ¨ç½²æ¨¡å¼
        if 'ä¸¤åœ°ä¸‰ä¸­å¿ƒ' in text or '2åœ°3ä¸­å¿ƒ' in text:
            deployment['deployment_mode'] = 'ä¸¤åœ°ä¸‰ä¸­å¿ƒ'
            deployment['remote_centers'] = 2
            deployment['same_city_centers'] = 2
        elif 'åŒåŸåŒä¸­å¿ƒ' in text or 'åŒåŸ2ä¸­å¿ƒ' in text:
            deployment['deployment_mode'] = 'åŒåŸåŒä¸­å¿ƒ'
            deployment['same_city_centers'] = 2
        elif 'åŒåŸå¤šä¸­å¿ƒ' in text:
            deployment['deployment_mode'] = 'åŒåŸå¤šä¸­å¿ƒ'
        elif 'ä¸‰åœ°äº”ä¸­å¿ƒ' in text or '3åœ°5ä¸­å¿ƒ' in text:
            deployment['deployment_mode'] = 'ä¸‰åœ°äº”ä¸­å¿ƒ'
        
        # è¯†åˆ«å®¹ç¾ç±»å‹
        if 'åŒæ´»' in text or 'active-active' in text_lower:
            deployment['disaster_recovery_type'] = 'åŒæ´»'
        elif 'å¤šæ´»' in text or 'multi-active' in text_lower:
            deployment['disaster_recovery_type'] = 'å¤šæ´»'
        
        # æå–RPO/RTO
        rpo_match = re.search(r'RPO[ï¼š:=\s]*(\d+)\s*(ç§’|åˆ†|å°æ—¶|s|m|h)', text, re.IGNORECASE)
        if rpo_match:
            deployment['rpo'] = rpo_match.group(1) + rpo_match.group(2)
        
        rto_match = re.search(r'RTO[ï¼š:=\s]*(\d+)\s*(ç§’|åˆ†|å°æ—¶|s|m|h)', text, re.IGNORECASE)
        if rto_match:
            deployment['rto'] = rto_match.group(1) + rto_match.group(2)
        
        return deployment
    
    def _calculate_statistics(self, systems: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        if not systems:
            return {}
        
        stats = {
            'total_systems': len(systems),
            'total_data_size_gb': 0,
            'total_qps': 0,
            'total_tps': 0,
            'total_connections': 0,
            'max_qps': 0,
            'avg_qps': 0
        }
        
        for system in systems:
            stats['total_data_size_gb'] += system.get('data_size_gb', 0)
            stats['total_qps'] += system.get('qps', 0)
            stats['total_tps'] += system.get('tps', 0)
            stats['total_connections'] += system.get('connections', 0)
            stats['max_qps'] = max(stats['max_qps'], system.get('qps', 0))
        
        if stats['total_systems'] > 0:
            stats['avg_qps'] = stats['total_qps'] / stats['total_systems']
        
        stats['total_data_size_tb'] = stats['total_data_size_gb'] / 1024
        
        return stats
    
    def _infer_deployment(self, systems: List[Dict[str, Any]], summary: Dict[str, Any]) -> Dict[str, Any]:
        """æ™ºèƒ½æ¨æ–­éƒ¨ç½²æ–¹å¼"""
        deployment = {
            'deployment_mode': 'å•ä¸­å¿ƒ',
            'disaster_recovery_type': 'å†·å¤‡'
        }
        
        # æ ¹æ®ç³»ç»Ÿæ•°é‡å’Œæ•°æ®é‡æ¨æ–­
        total_systems = len(systems)
        total_data_gb = sum(s.get('data_size_gb', 0) for s in systems)
        max_qps = max((s.get('qps', 0) for s in systems), default=0)
        
        # å¤§è§„æ¨¡ç³»ç»Ÿå»ºè®®å¤šä¸­å¿ƒ
        if total_systems > 10 or total_data_gb > 10000 or max_qps > 50000:
            deployment['deployment_mode'] = 'ä¸¤åœ°ä¸‰ä¸­å¿ƒ'
            deployment['disaster_recovery_type'] = 'åŒæ´»'
        elif total_systems > 5 or total_data_gb > 5000 or max_qps > 20000:
            deployment['deployment_mode'] = 'åŒåŸåŒä¸­å¿ƒ'
            deployment['disaster_recovery_type'] = 'çƒ­å¤‡'
        
        return deployment
    
    def _map_columns(self, columns: List[str]) -> Dict[str, str]:
        """æ˜ å°„åˆ—ååˆ°æ ‡å‡†å­—æ®µ"""
        mapping = {}
        
        for col in columns:
            col_lower = str(col).lower()
            
            if 'ç³»ç»Ÿåç§°' in col or 'system name' in col_lower or 'ç³»ç»Ÿ' == col:
                mapping[col] = 'system_name'
            elif 'ç³»ç»Ÿç±»å‹' in col or 'system type' in col_lower:
                mapping[col] = 'system_type'
            elif 'ä¸šåŠ¡æ¨¡å—' in col or 'business' in col_lower or 'æ¨¡å—' in col:
                mapping[col] = 'business_module'
            elif 'æ•°æ®é‡' in col or 'data size' in col_lower or 'å®¹é‡' in col:
                mapping[col] = 'data_size_gb'
            elif 'è¡¨æ•°' in col or 'table count' in col_lower:
                mapping[col] = 'table_count'
            elif 'qps' in col_lower:
                if 'å³°å€¼' in col or 'peak' in col_lower:
                    mapping[col] = 'peak_qps'
                else:
                    mapping[col] = 'qps'
            elif 'tps' in col_lower:
                mapping[col] = 'tps'
            elif 'è¿æ¥' in col or 'connection' in col_lower:
                mapping[col] = 'connections'
            elif 'å¯ç”¨æ€§' in col or 'availability' in col_lower:
                mapping[col] = 'availability_requirement'
            elif 'æ•æ„Ÿ' in col or 'sensitivity' in col_lower:
                mapping[col] = 'data_sensitivity'
            elif 'å¤‡ä»½' in col or 'backup' in col_lower:
                mapping[col] = 'backup_requirement'
            elif 'å¤‡æ³¨' in col or 'note' in col_lower or 'è¯´æ˜' in col:
                mapping[col] = 'notes'
        
        return mapping
    
    def _clean_value(self, value: Any, field_name: str) -> Any:
        """æ¸…æ´—å’Œè½¬æ¢å€¼"""
        if pd.isna(value):
            return None
        
        value_str = str(value).strip()
        
        # æ•°å€¼ç±»å‹å­—æ®µ
        if field_name in ['data_size_gb', 'qps', 'tps', 'peak_qps', 'connections', 'table_count']:
            return self._parse_number(value_str)
        
        # å¤§å°å­—æ®µ
        if field_name == 'data_size_gb':
            return self._parse_size(value_str)
        
        return value_str
    
    def _parse_number(self, value: str) -> float:
        """è§£ææ•°å­—"""
        # ç§»é™¤é€—å·å’Œç©ºæ ¼
        value = re.sub(r'[,\s]', '', str(value))
        
        # æå–æ•°å­—
        match = re.search(r'[\d.]+', value)
        if match:
            try:
                return float(match.group())
            except:
                return 0
        return 0
    
    def _parse_size(self, value: str) -> float:
        """è§£æå¤§å°ï¼ˆè½¬æ¢ä¸ºGBï¼‰"""
        value = str(value).upper()
        
        # æå–æ•°å­—
        match = re.search(r'([\d.]+)\s*([KMGTP]?B?)', value)
        if match:
            num = float(match.group(1))
            unit = match.group(2)
            
            # è½¬æ¢ä¸ºGB
            if 'TB' in unit or 'T' == unit:
                return num * 1024
            elif 'GB' in unit or 'G' == unit:
                return num
            elif 'MB' in unit or 'M' == unit:
                return num / 1024
            elif 'KB' in unit or 'K' == unit:
                return num / (1024 * 1024)
            elif 'PB' in unit or 'P' == unit:
                return num * 1024 * 1024
            else:
                return num
        
        return 0
    
    def _preprocess_image(self, image: Image.Image) -> Image.Image:
        """å›¾åƒé¢„å¤„ç†"""
        # è½¬æ¢ä¸ºç°åº¦å›¾
        if image.mode != 'L':
            image = image.convert('L')
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        img_array = np.array(image)
        
        # äºŒå€¼åŒ–
        _, img_binary = cv2.threshold(img_array, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        
        # å»å™ª
        img_denoised = cv2.fastNlMeansDenoising(img_binary)
        
        return Image.fromarray(img_denoised)
    
    def _extract_tables_from_image(self, image: Image.Image) -> List[Dict[str, Any]]:
        """ä»å›¾åƒæå–è¡¨æ ¼"""
        # è¿™é‡Œå¯ä»¥ä½¿ç”¨æ›´é«˜çº§çš„è¡¨æ ¼æ£€æµ‹ç®—æ³•
        # ç®€åŒ–ç‰ˆæœ¬ï¼šè¿”å›ç©ºåˆ—è¡¨
        return []


# æµ‹è¯•ä»£ç 
if __name__ == '__main__':
    processor = AdvancedFileProcessor()
    
    print("=" * 60)
    print("ğŸš€ é«˜çº§æ–‡ä»¶å¤„ç†å™¨æµ‹è¯•")
    print("=" * 60)
    print(f"âœ… æ”¯æŒæ ¼å¼: {list(processor.supported_formats.keys())}")
    print(f"âœ… Excelæ”¯æŒ: {EXCEL_AVAILABLE}")
    print(f"âœ… PDFæ”¯æŒ: {PDF_AVAILABLE}")
    print(f"âœ… å›¾åƒæ”¯æŒ: {IMAGE_AVAILABLE}")
    print(f"âœ… EasyOCRæ”¯æŒ: {EASYOCR_AVAILABLE}")
    print("=" * 60)
